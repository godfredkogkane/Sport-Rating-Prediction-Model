# -*- coding: utf-8 -*-
"""Group12._SportsPrediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YW4m5cNk19yNm1Oet2uP3fR8Z9fEmO4z

The purpose of this code is to clean, impute and encode a dataset containing some features for rating a player's performance by FIFA in 2021, use that data to train an Artificial Intelligence model to predict a player's rating, and test that model using the 2022 dataset.

---
"""

# Importing relevant libraries and connecting the file to Google Drive
import pandas as pd
import numpy as np
import joblib
from sklearn.impute import SimpleImputer
from xgboost import XGBRegressor
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split, cross_val_score, KFold, GridSearchCV
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

from google.colab import drive
drive.mount('/content/drive')

# Reading the dataset
df = pd.read_csv('/content/drive/My Drive/AI/players_21.csv')

# Displaying the first five rows in the dataset
df.head()

# Viewing the description of the data in the dataset
df.describe()

# Creating a list of columns that would be unneccessary in the prediction
columns_to_drop=['sofifa_id','player_url','short_name','dob','long_name','club_team_id',
                 'club_position','club_flag_url','club_loaned_from','club_joined',
                 'club_contract_valid_until','nationality_id','nation_flag_url',
                 'nation_logo_url','club_logo_url','player_face_url','nationality_name',
                 'nation_team_id','nation_position','nation_jersey_number','real_face',
                 'ls','st','rs','lw','lf','cf','rf','rw','lam','cam','ram','lm','lcm','cm','rcm','rm',
                 'lwb','ldm','cdm','rdm','rwb','lb','lcb','cb','rcb','rb','gk']

"""Justification for dropping the listed columns when predicting a player's rating:
**sofifa_id, player_url:** These columns are unique identifiers and contain no inherent information about a player's skill or performance. They are only for administrative purposes and thus irrelevant for predicting ratings.

**short_name, dob, long_name:** A player's name, date of birth, and full name do not directly impact their abilities or performance on the field. Including these personal details in the model would introduce noise and complexity without adding predictive value.

**club_team_id:** Information about a player's club is gotten from their club name. As such, the team ID is not relevant to their rating.

**club_position,club_flag_url, club_loaned_from, club_joined, club_contract_valid_until:** While these columns provide contextual information about a player's team and role, they are not measures of a player's inherent talent or potential. Including them could lead to overfitting based on transient factors.

**nationality_id, nation_flag_url, nation_logo_url, nationality_name, nation_team_id, nation_position, nation_jersey_number:** These columns describe a player's nationality and involvement with national teams, but nationality does not correlate with a player's performance or rating. These are social and administrative attributes, not performance indicators.

**player_face_url, real_face:** The player's appearance or facial features are unrelated to their athletic abilities or rating. Including visual aspects would be irrelevant to predicting their performance.

**Columns like ls, st, rs, lw, lf, cf, rf, rw, lam, cam, ram, lm, lcm, cm, rcm, rm, lwb, ldm, cdm, rdm, rwb, lb, lcb, cb, rcb, rb:**  While these attributes provide detailed information about a player's positions and skills, including all of them could result in overfitting and a more complex model. By selecting the most relevant features, we ensure the model focuses on the attributes that truly influence a player's rating. These features are also mentioned in the player_positions column.
"""

# Dropping the unneccessary columns using the list
df.drop(columns_to_drop, axis=1, inplace=True)

df.head()

# Imputing missing values
imputer = SimpleImputer(strategy='most_frequent')
df_imputed = imputer.fit_transform(df)

# Converting the imputed array back to a DataFrame
df_imputed = pd.DataFrame(df_imputed, columns=df.columns)

# Getting the categorical columns
cat_cols = df_imputed.select_dtypes(include=['object']).columns.tolist()
cat_cols

# Encoding the object columns using the LabelEncoder
label_encoder = LabelEncoder()
for col in cat_cols:
    df_imputed[col] = label_encoder.fit_transform(df_imputed[col])

# Ensuring that the columns of type object are now of type int or float
df_imputed.info()

# Splitting the data into features (X) and target variable (y)
X = df_imputed.drop(columns=['overall'])
y = df_imputed['overall']

# Feature selection of features with a rating greater than 0.4
corr_matrix = df_imputed.corr()
corr_overall_rating = abs(corr_matrix['overall'])
relevant_features = corr_overall_rating[corr_overall_rating > 0.4]
relevant_features.drop(['overall'], inplace=True)
X = df_imputed[relevant_features.index]
y = df_imputed['overall']

# Scaling the data
scaler = StandardScaler()
scaler.fit_transform(X)

# Saving relevant features to a CSV file
relevant_features.to_csv('relevant_features.csv', index=False)

print(relevant_features.sort_values(ascending=False))

# Creating an XGBoostRegressor model
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
model = XGBRegressor(random_state=42)

# Defining the number of splits for K-Fold cross-validation
n_splits = 5

kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)

# Lists to store the results
mse_scores = []
mae_scores = []
r2_scores = []

for train_index, test_index in kf.split(X_train):
    X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[test_index]
    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[test_index]

    model.fit(X_train_fold, y_train_fold)
    y_pred = model.predict(X_val_fold)

    mse = mean_squared_error(y_val_fold, y_pred)
    mae = mean_absolute_error(y_val_fold, y_pred)
    r2 = r2_score(y_val_fold, y_pred)

    mse_scores.append(mse)
    mae_scores.append(mae)
    r2_scores.append(r2)

print("XGBoost Regressor Cross-validation Mean Squared Error:", np.mean(mse_scores))
print("XGBoost Regressor Cross-validation Mean Absolute Error:", np.mean(mae_scores))
print("XGBoost Regressor Cross-validation R-squared score:", np.mean(r2_scores))

"""**The below is Random Forest Regressor that is implemented using the RandomForestRegressor class from the sklearn.ensemble module. It creates an ensemble of decision trees and uses bagging to improve the overall performance.**"""

# Creating a RandomForestRegressor model
model = RandomForestRegressor(random_state=42)

# Defining the number of splits for K-Fold cross-validation
n_splits = 5  # You can adjust this as needed

kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)

# Lists to store the results
mse_scores = []
mae_scores = []
r2_scores = []

for train_index, test_index in kf.split(X_train):
    X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[test_index]
    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[test_index]

    model.fit(X_train_fold, y_train_fold)
    y_pred = model.predict(X_val_fold)

    mse = mean_squared_error(y_val_fold, y_pred)
    mae = mean_absolute_error(y_val_fold, y_pred)
    r2 = r2_score(y_val_fold, y_pred)

    mse_scores.append(mse)
    mae_scores.append(mae)
    r2_scores.append(r2)

print("Random Forest Regressor Cross-validation Mean Squared Error:", np.mean(mse_scores))
print("Random Forest Regressor Cross-validation Mean Absolute Error:", np.mean(mae_scores))
print("Random Forest Regressor Cross-validation R-squared score:", np.mean(r2_scores))

# Creating a GradientBoostingRegressor model
model = GradientBoostingRegressor(random_state=42)

# Defining the number of splits for K-Fold cross-validation
n_splits = 5  # You can adjust this as needed

kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)

# Lists to store the results
mse_scores = []
mae_scores = []
r2_scores = []

for train_index, test_index in kf.split(X_train):
    X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[test_index]
    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[test_index]

    model.fit(X_train_fold, y_train_fold)
    y_pred = model.predict(X_val_fold)

    mse = mean_squared_error(y_val_fold, y_pred)
    mae = mean_absolute_error(y_val_fold, y_pred)
    r2 = r2_score(y_val_fold, y_pred)

    mse_scores.append(mse)
    mae_scores.append(mae)
    r2_scores.append(r2)

print("Gradient Boosting Regressor Cross-validation Mean Squared Error:", np.mean(mse_scores))
print("Gradient Boosting Regressor Cross-validation Mean Absolute Error:", np.mean(mae_scores))
print("Gradient Boosting Regressor Cross-validation R-squared score:", np.mean(r2_scores))

# Creating the AdaBoostRegressor with a base estimator (i.e. DecisionTreeRegressor)
dt = DecisionTreeRegressor(random_state=42)
ab = AdaBoostRegressor(estimator=dt, random_state=42)

cv = KFold(n_splits=2)

# Defining the hyperparameter grid to search
PARAMETERS = {
    'n_estimators': [50, 100, 200],
    'learning_rate': [0.01, 0.1, 1.0],
}
# Creating the GridSearchCV object
model_gs = GridSearchCV(ab, param_grid=PARAMETERS, cv=cv, scoring="neg_mean_squared_error")
model_gs.fit(X_train, y_train)

y_pred = model_gs.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print('Grid Search Mean Squared Error:', mse)
print('Grid Search Regressor Mean Absolute Error:', mae)
print('Grid Search Regressor R-squared score:', r2)

# Getting the best parameters and best estimator
best_params = model_gs.best_params_
best_regressor = model_gs.best_estimator_

# Using the best regressor for predictions
y_pred = best_regressor.predict(X_test)


# Save the trained model
joblib.dump(best_regressor, 'trained_model.pkl')

# Save the scaler
joblib.dump(scaler, 'scaler.pkl')

"""We chose the Ada Boost Regressor with Grid Search CV as the best model becuase of the following:

Mean Squared Error (MSE): The MSE measures the average squared difference between the predicted and actual target values. In our case, the MSE is relatively low (approximately 0.21), which means that, on average, the model's predictions are very close to the actual values. Lower MSE values indicate better predictive accuracy.

Mean Absolute Error (MAE): The MAE measures the average absolute difference between predicted and actual values. A low MAE (around 0.157) indicates that the model's predictions are, on average, close to the actual values. MAE is a more robust metric than MSE to the influence of outliers.

R-squared (R2) score: The R2 score measures the proportion of the variance in the target variable that is predictable from the features. An R2 score close to 1 (0.996 in our case) indicates that the model explains a large portion of the variance in the target variable. In other words, the Ada Boost model captures most of the patterns and relationships within the data, making it a strong predictor.

**The below is Gradient Boosting Regressor that is implemented using the GradientBoostingRegressor class, which creates an ensemble of decision trees and uses boosting to iteratively improve the model's performance.**
"""

# Reading the 2022 dataset for testing the model
df_new_season = pd.read_csv('/content/drive/My Drive/AI/players_22.csv')

df_new_season.head()

# Creating a list of columns that would be unneccessary in the prediction
columns_to_drop_2=['sofifa_id','player_url','short_name','dob','long_name','club_team_id','club_name',
                 'league_name','league_level','club_position','club_jersey_number','club_flag_url',
                 'club_loaned_from','club_joined','club_contract_valid_until','nationality_id',
                 'nation_flag_url','nation_logo_url','club_logo_url','player_face_url','nationality_name',
                 'nation_team_id','nation_position','nation_jersey_number','real_face','player_tags',
                 'ls','st','rs','lw','lf','cf','rf','rw','lam','cam','ram','lm','lcm','cm','rcm','rm',
                 'lwb','ldm','cdm','rdm','rwb','lb','lcb','cb','rcb','rb','gk']

# Dropping the unneccessary columns using the list
df_new_season.drop(columns_to_drop_2, axis=1, inplace=True)

# Imputing the new dataset
imputer = SimpleImputer(strategy='most_frequent')
df_imputed_new_season = imputer.fit_transform(df_new_season)

# Converting the imputed array back to a DataFrame
df_imputed_new_season = pd.DataFrame(df_imputed, columns=df.columns)

# Getting the categorical columns
cat_cols_2 = df_new_season.select_dtypes(include=['object']).columns.tolist()
cat_cols_2

# Label encoding the categorical columns in the new dataset
label_encoder = LabelEncoder()
for col in cat_cols_2:
    df_imputed_new_season[col] = label_encoder.fit_transform(df_imputed_new_season[col])

# Selecting the independent and dependent variables
X_new_season = df_imputed_new_season[relevant_features.index]
y_new_season = df_imputed_new_season['overall']

# Using the trained model to make predictions on the new season's data
y_pred_new_season = model_gs.predict(X_new_season)

# Saving the y_test and y_pred values in a CSV file
data = {'y_test': y_new_season, 'y_pred': y_pred_new_season}
new_df = pd.DataFrame(data)
new_df.to_csv('y_test_and_y_pred.csv', index=False)

# Evaluating the model's performance on the new data
mse_new_season = mean_squared_error(y_new_season, y_pred_new_season)
mae_new_season = mean_absolute_error(y_new_season, y_pred_new_season)
r2_new_season = r2_score(y_new_season, y_pred_new_season)
print('Mean Squared Error (New Season):', mse_new_season)
print('Mean Absolute Error (New Season):', mae_new_season)
print('R-squared score (New Season):', r2_new_season)